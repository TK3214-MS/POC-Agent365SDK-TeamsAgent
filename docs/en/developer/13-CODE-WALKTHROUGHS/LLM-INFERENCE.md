# LLM Inference - Detailed LLM Inference Process

[![æ—¥æœ¬èª](https://img.shields.io/badge/lang-æ—¥æœ¬èª-red.svg)](../../../developer/13-CODE-WALKTHROUGHS/LLM-INFERENCE.md)
[![English](https://img.shields.io/badge/lang-English-blue.svg)](LLM-INFERENCE.md)

## ğŸ“‹ Inference Flow

### 1. Inference with IChatClient

```csharp
var chatClient = _llmProvider.GetChatClient();

var messages = new List<ChatMessage>
{
    new ChatMessage(ChatRole.System, SystemPrompt),
    new ChatMessage(ChatRole.User, "ä»Šé€±ã®å•†è«‡ã‚µãƒãƒªã‚’æ•™ãˆã¦ãã ã•ã„")
};

var options = new ChatOptions
{
    Temperature = 0.7f,
    MaxTokens = 2000,
    Tools = new List<AITool>
    {
        AIFunctionFactory.Create(_emailTool.SearchSalesEmails),
        AIFunctionFactory.Create(_calendarTool.SearchSalesMeetings)
    }
};

var response = await chatClient.CompleteAsync(messages, options);
```

---

## Azure OpenAI Inference Details

### Request Construction

**Internally generated HTTP request**:

```http
POST https://<resource-name>.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-02-01
Content-Type: application/json
api-key: <api-key>

{
  "messages": [
    {
      "role": "system",
      "content": "ã‚ãªãŸã¯å–¶æ¥­æ”¯æ´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦..."
    },
    {
      "role": "user",
      "content": "ä»Šé€±ã®å•†è«‡ã‚µãƒãƒªã‚’æ•™ãˆã¦ãã ã•ã„"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 2000,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "SearchSalesEmails",
        "description": "å•†è«‡é–¢é€£ã®ãƒ¡ãƒ¼ãƒ«ã‚’æ¤œç´¢ã—ã¦å–å¾—ã—ã¾ã™",
        "parameters": {
          "type": "object",
          "properties": {
            "startDate": {
              "type": "string",
              "description": "æ¤œç´¢é–‹å§‹æ—¥ (yyyy-MM-dd)"
            },
            "endDate": {
              "type": "string",
              "description": "æ¤œç´¢çµ‚äº†æ—¥ (yyyy-MM-dd)"
            },
            "keywords": {
              "type": "string",
              "description": "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
              "default": "å•†è«‡,ææ¡ˆ,è¦‹ç©,å¥‘ç´„"
            }
          },
          "required": ["startDate", "endDate"]
        }
      }
    }
  ]
}
```

---

## Tool Calling Flow

### Step 1: Initial LLM Call

**LLM Response** (tool call):

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1707123456,
  "model": "gpt-4",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "call_email_search_1",
            "type": "function",
            "function": {
              "name": "SearchSalesEmails",
              "arguments": "{\"startDate\":\"2026-02-03\",\"endDate\":\"2026-02-09\",\"keywords\":\"å•†è«‡,ææ¡ˆ\"}"
            }
          }
        ]
      },
      "finish_reason": "tool_calls"
    }
  ]
}
```

### Step 2: Tool Execution

**C# Code**:

```csharp
// Automatically handled by FunctionInvocation Middleware
var toolCall = response.Message.ToolCalls[0];
var functionName = toolCall.Function.Name;        // "SearchSalesEmails"
var arguments = toolCall.Function.Arguments;      // {"startDate":"2026-02-03",...}

// JSON deserialization
var args = JsonSerializer.Deserialize<SearchSalesEmailsArgs>(arguments);

// Tool execution
var result = await _emailTool.SearchSalesEmails(
    args.StartDate,
    args.EndDate,
    args.Keywords
);

// Result: "ğŸ“§ å•†è«‡é–¢é€£ãƒ¡ãƒ¼ãƒ« (5ä»¶)..."
```

### Step 3: Return Tool Results to LLM

**Extended message list**:

```csharp
messages.Add(new ChatMessage(ChatRole.Assistant)
{
    ToolCalls = new[] { toolCall }
});

messages.Add(new ChatMessage(ChatRole.Tool)
{
    ToolCallId = "call_email_search_1",
    Content = result  // "ğŸ“§ å•†è«‡é–¢é€£ãƒ¡ãƒ¼ãƒ« (5ä»¶)..."
});
```

**Second LLM request**:

```json
{
  "messages": [
    {
      "role": "system",
      "content": "ã‚ãªãŸã¯å–¶æ¥­æ”¯æ´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™..."
    },
    {
      "role": "user",
      "content": "ä»Šé€±ã®å•†è«‡ã‚µãƒãƒªã‚’æ•™ãˆã¦ãã ã•ã„"
    },
    {
      "role": "assistant",
      "tool_calls": [
        {
          "id": "call_email_search_1",
          "type": "function",
          "function": {
            "name": "SearchSalesEmails",
            "arguments": "{\"startDate\":\"2026-02-03\",...}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "tool_call_id": "call_email_search_1",
      "content": "ğŸ“§ å•†è«‡é–¢é€£ãƒ¡ãƒ¼ãƒ« (5ä»¶)..."
    }
  ]
}
```

### Step 4: Additional Tool Call (Calendar)

**LLM Response**:

```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "tool_calls": [
          {
            "id": "call_calendar_search_1",
            "function": {
              "name": "SearchSalesMeetings",
              "arguments": "{\"startDate\":\"2026-02-03\",\"endDate\":\"2026-02-09\"}"
            }
          }
        ]
      },
      "finish_reason": "tool_calls"
    }
  ]
}
```

**Tool execution**:

```csharp
var result = await _calendarTool.SearchSalesMeetings(
    "2026-02-03",
    "2026-02-09",
    "å•†è«‡,ææ¡ˆ"
);
// Result: "ğŸ“… å•†è«‡äºˆå®š (3ä»¶)..."
```

### Step 5: Final Summary Generation

**Third LLM request** (including 2 tool results):

```json
{
  "messages": [
    /* System prompt */,
    /* User query */,
    /* Email search tool call */,
    /* Email search result */,
    /* Calendar search tool call */,
    /* Calendar search result */
  ]
}
```

**Final LLM response**:

```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "## ğŸ“Š ã‚µãƒãƒªãƒ¼\nä»Šé€±ã¯5ä»¶ã®å•†è«‡ãƒ¡ãƒ¼ãƒ«ã¨3ä»¶ã®äºˆå®šãŒã‚ã‚Šã¾ã™ã€‚\n\n## ğŸ“§ å•†è«‡ãƒ¡ãƒ¼ãƒ«\n- æ ªå¼ä¼šç¤¾Aç¤¾ã‹ã‚‰ã®ææ¡ˆä¾é ¼ï¼ˆ2/5å—ä¿¡ï¼‰\n- Bç¤¾è¦‹ç©ã‚‚ã‚Šé€ä»˜å®Œäº†ï¼ˆ2/6é€ä¿¡ï¼‰\n\n## ğŸ“… å•†è«‡äºˆå®š\n- 2/5 14:00 æ ªå¼ä¼šç¤¾Aç¤¾ å•†è«‡\n- 2/7 10:00 Bç¤¾ è¦‹ç©èª¬æ˜\n\n## ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n1. Aç¤¾ææ¡ˆæ›¸ã‚’2/4ã¾ã§ã«æº–å‚™\n2. Bç¤¾è¦‹ç©ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 1250,
    "completion_tokens": 350,
    "total_tokens": 1600
  }
}
```

---

## Parameter Tuning

### Temperature

```csharp
// Low temperature (0.0-0.3): Deterministic, consistency-focused
Temperature = 0.2f  // Business reports, summaries

// Medium temperature (0.4-0.7): Balanced
Temperature = 0.7f  // Conversations, summary generation

// High temperature (0.8-1.0): Creative, diverse
Temperature = 0.9f  // Brainstorming, idea generation
```

### MaxTokens

```csharp
// Concise response
MaxTokens = 500  // Approximately 200-300 characters

// Standard summary
MaxTokens = 1500  // 500-700 characters

// Detailed report
MaxTokens = 4000  // 2000+ characters
```

### TopP (Nucleus Sampling)

```csharp
// Alternative to Temperature
TopP = 0.95f  // Select from words in the top 95% probability
```

---

## Streaming Response

### CompleteStreamingAsync

```csharp
await foreach (var update in chatClient.CompleteStreamingAsync(messages, options))
{
    if (update.Text != null)
    {
        // Display to user immediately
        await turnContext.SendActivityAsync(update.Text);
    }
    
    if (update.FinishReason == ChatFinishReason.ToolCalls)
    {
        // Tool call detected
        foreach (var toolCall in update.ToolCalls)
        {
            var result = await ExecuteToolAsync(toolCall);
            messages.Add(new ChatMessage(ChatRole.Tool, result));
        }
        
        // Re-invoke LLM with tool results
        await foreach (var finalUpdate in chatClient.CompleteStreamingAsync(messages, options))
        {
            await turnContext.SendActivityAsync(finalUpdate.Text);
        }
    }
}
```

**User experience improvement**:
```
Non-streaming:
  User input â†’ [3 sec wait] â†’ Full response displayed

Streaming:
  User input â†’ [0.5 sec] â†’ "## ğŸ“Š" â†’ "ã‚µãƒãƒªãƒ¼\n" â†’ "ä»Šé€±ã¯..." â†’ ...
                  Real-time display
```

---

## Error Handling
